## Provide a name in place of prometheus-operator for `app:` labels
nameOverride: ""

## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
kubeTargetVersionOverride: ""

## Provide a name to substitute for the full names of resources
fullnameOverride: ""

## Labels to apply to all resources
commonLabels: {}

## Create default rules for monitoring the cluster
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    general: true
    k8s: true
    kubeApiserver: true
    kubeApiserverError: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesAbsent: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: true
    network: true
    node: true
    prometheus: true
    prometheusOperator: true
    time: true

  ## Runbook url prefix for default rules
  runbookUrl: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#
  ## Reduce app namespace alert scope
  appNamespacesTarget: ".*"

  ## Labels for default rules
  labels: {}
  ## Annotations for default rules
  annotations: {}

## Provide custom recording or alerting rules to be deployed into the cluster.
additionalPrometheusRules: []

global:
  rbac:
    create: true
    pspEnabled: true

  ## Reference to one or more secrets to be used when pulling images
  imagePullSecrets: []

## Configuration for alertmanager
alertmanager:

  ## Deploy alertmanager
  enabled: true

  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
  apiVersion: v2

  ## Service account for Alertmanager to use.
  serviceAccount:
    create: true
    name: ""

  ## Configure pod disruption budgets for Alertmanager
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  ## Alertmanager configuration directives
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
    receivers:
    - name: 'null'

  tplConfig: false

  ## Alertmanager template files to format alerts
  templateFiles: {}

  ingress:
    enabled: false

    annotations: {}

    labels: {}

    ## Hosts must be provided if Ingress is enabled.
    hosts: []

    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
    paths: []

    ## TLS configuration for Alertmanager Ingress
    ## Secret must be manually created in the namespace
    tls: []

  ## Configuration for Alertmanager secret
  secret:
    annotations: {}

  ## Configuration for Alertmanager service
  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Alertmanager Service to listen on
    port: 9093
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    nodePort: 30903
    ## List of IP addresses at which the Prometheus server service is available
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    ## Service type
    type: ClusterIP

  ## If true, create a serviceMonitor for alertmanager
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    selfMonitor: true

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []

    # 	relabel configs to apply to samples before ingestion.
    relabelings: []

  ## Settings affecting alertmanagerSpec
  alertmanagerSpec:

    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
    podMetadata: {}

    ## Image of Alertmanager
    image:
      repository: quay.io/prometheus/alertmanager
      tag: v0.20.0

    ## If true then the user will be responsible to provide a secret with alertmanager configuration
    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
    useExistingSecret: false

    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
    secrets: []

    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
    configMaps: []

    ## Define Log Format
    # Use logfmt (default) or json-formatted logging
    logFormat: logfmt

    ## Log level for Alertmanager to be configured with.
    logLevel: info

    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
    ## running cluster equal to the expected size.
    replicas: 1

    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    retention: 120h

    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    storage: {}

    ## 	The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name.	string	false
    externalUrl:

    ## 	The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
    routePrefix: /

    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
    paused: false

    ## Define which Nodes the Pods are scheduled on.
    nodeSelector: {}

    ## Define resources requests and limits for single Pods.
    resources: {}

    podAntiAffinity: ""

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Assign custom affinity rules to the alertmanager instance
    affinity: {}

    ## If specified, the pod's tolerations.
    tolerations: []

    ## SecurityContext holds pod-level security attributes and common container settings.
    ## This defaults to non root user with uid 1000 and gid 2000.	*v1.PodSecurityContext	false
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000

    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
    ## Note this is only for the Alertmanager UI, not the gossip communication.
    listenLocal: false

    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
    containers: []

    ## Priority class assigned to the Pods
    priorityClassName: ""

    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
    additionalPeers: []

    ## PortName to use for Alert Manager.
    portName: "web"

grafana:
  enabled: true

  ## Deploy default dashboards.
  defaultDashboardsEnabled: true

  adminPassword: WElcome_#12

  ingress:
    ## If true, Grafana Ingress will be created
    enabled: false

    ## Annotations for Grafana Ingress
    annotations: {}

    ## Labels to be added to the Ingress
    labels: {}

    hosts: []

    ## Path for grafana ingress
    path: /

    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    tls: []

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
    datasources:
      enabled: true
      defaultDatasourceEnabled: true

      ## Annotations for Grafana datasource configmaps
      annotations: {}

      ## Create datasource for each Pod of Prometheus StatefulSet;
      ## this uses headless service `prometheus-operated` which is
      createPrometheusReplicasDatasources: false
      label: grafana_datasource

  extraConfigmapMounts: []

  ## Configure additional grafana datasources
  additionalDataSources: []

  ## Passed to grafana subchart and used by servicemonitor below
  service:
    portName: service

  ## If true, create a serviceMonitor for grafana
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    selfMonitor: true

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []

    # 	relabel configs to apply to samples before ingestion.
    relabelings: []

## Component scraping the kube api server
kubeApiServer:
  enabled: true
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: false

  ## If your API endpoint address is not reachable (as in AKS) you can replace it with the kubernetes service
  relabelings: []

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []

## Component scraping the kubelet and kubelet-hosted cAdvisor
kubelet:
  enabled: true
  namespace: kube-system

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## Enable scraping the kubelet over https. For requirements to enable this see
    https: true

    ## Metric relabellings to apply to samples before ingestion
    cAdvisorMetricRelabelings: []
    cAdvisorRelabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path

    metricRelabelings: []

    relabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path

## Component scraping the kube controller manager
kubeControllerManager:
  enabled: true

  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
  endpoints: []

  ## If using kubeControllerManager.endpoints only the port and targetPort are used
  service:
    port: 10252
    targetPort: 10252

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    https: false

    # Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    # Name of the server to use when validating TLS certificate
    serverName: null

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []

    # 	relabel configs to apply to samples before ingestion.
    relabelings: []

## Component scraping coreDns. Use either this or kubeDns
coreDns:
  enabled: true
  service:
    port: 9153
    targetPort: 9153
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []

    # 	relabel configs to apply to samples before ingestion.
    relabelings: []

## Component scraping kubeDns. Use either this or coreDns

kubeDns:
  enabled: false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []
    dnsmasqMetricRelabelings: []
    dnsmasqRelabelings: []

## Component scraping etcd
kubeEtcd:
  enabled: true

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  endpoints: []

  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  service:
    port: 2379
    targetPort: 2379
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    scheme: http
    insecureSkipVerify: false
    serverName: ""
    caFile: ""
    certFile: ""
    keyFile: ""

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []

## Component scraping kube scheduler
kubeScheduler:
  enabled: true

  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
  endpoints: []

  ## If using kubeScheduler.endpoints only the port and targetPort are used
  service:
    port: 10251
    targetPort: 10251

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## Enable scraping kube-scheduler over https.
    https: false

    ## Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    ## Name of the server to use when validating TLS certificate
    serverName: null

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []

## Component scraping kube proxy
kubeProxy:
  enabled: true

  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
  endpoints: []

  service:
    port: 10249
    targetPort: 10249

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## Enable scraping kube-proxy over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    https: false

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []

## Component scraping kube state metrics
kubeStateMetrics:
  enabled: true
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []

## Configuration for kube-state-metrics subchart
kube-state-metrics:
  rbac:
    create: true
  podSecurityPolicy:
    enabled: true

## Deploy node exporter as a daemonset to all nodes
nodeExporter:
  enabled: true

  ## Use the value configured in prometheus-node-exporter.podLabels
  jobLabel: jobLabel

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""

    ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
    scrapeTimeout: ""

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []

## Configuration for prometheus-node-exporter subchart
prometheus-node-exporter:
  podLabels:
    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$

## Manages Prometheus and Alertmanager components
prometheusOperator:
  enabled: true

  # If true prometheus operator will create and update its CRDs on startup
  manageCrds: true

  tlsProxy:
    enabled: true
    image:
      repository: squareup/ghostunnel
      tag: v1.5.2
      pullPolicy: IfNotPresent
    resources: {}

  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
  ## rules from making their way into prometheus and potentially preventing the container from starting
  admissionWebhooks:
    failurePolicy: Fail
    enabled: true
    patch:
      enabled: true
      image:
        repository: jettech/kube-webhook-certgen
        tag: v1.0.0
        pullPolicy: IfNotPresent
      resources: {}
      ## Provide a priority class name to the webhook patching job
      priorityClassName: ""
      podAnnotations: {}
      nodeSelector: {}
      affinity: {}
      tolerations: []

  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
  namespaces: {}

  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
  denyNamespaces: []

  ## Service account for Alertmanager to use.
  serviceAccount:
    create: true
    name: ""

  ## Configuration for Prometheus operator service
  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    nodePort: 30080

    nodePortTls: 30443

  ## Additional ports to open for Prometheus service
    additionalPorts: []

  ## Loadbalancer IP
  ## Only use if service.type is "loadbalancer"
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ## Service type
  ## NodePort, ClusterIP, loadbalancer
    type: ClusterIP

    ## List of IP addresses at which the Prometheus server service is available
    externalIPs: []

  ## Deploy CRDs used by Prometheus Operator.
  createCustomResource: true

  ## Attempt to clean up CRDs created by Prometheus Operator.
  cleanupCustomResource: false

  ## Labels to add to the operator pod
  podLabels: {}

  ## Annotations to add to the operator pod
  podAnnotations: {}

  kubeletService:
    enabled: true
    namespace: kube-system

  ## Create a servicemonitor for the operator
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    selfMonitor: true

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []
    relabelings: []

  ## Resource limits & requests
  resources: {}
  nodeSelector: {}

  ## Tolerations for use with node taints
  tolerations: []
  affinity: {}
  securityContext:
    runAsNonRoot: true
    runAsUser: 65534

  ## Prometheus-operator image
  image:
    repository: quay.io/coreos/prometheus-operator
    tag: v0.36.0
    pullPolicy: IfNotPresent

  ## Configmap-reload image to use for reloading configmaps
  configmapReloadImage:
    repository: quay.io/coreos/configmap-reload
    tag: v0.0.1

  ## Prometheus-config-reloader image to use for config and rule reloading
  prometheusConfigReloaderImage:
    repository: quay.io/coreos/prometheus-config-reloader
    tag: v0.36.0

  ## Set the prometheus config reloader side-car CPU limit
  configReloaderCpu: 100m

  ## Set the prometheus config reloader side-car memory limit
  configReloaderMemory: 25Mi

  ## Hyperkube image to use when cleaning up
  hyperkubeImage:
    repository: k8s.gcr.io/hyperkube
    tag: v1.12.1
    pullPolicy: IfNotPresent

## Deploy a Prometheus instance
prometheus:

  enabled: true

  ## Annotations for Prometheus
  annotations: {}

  ## Service account for Prometheuses to use.
  serviceAccount:
    create: true
    name: ""

  ## Configuration for Prometheus service
  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Prometheus Service to listen on
    port: 9090

    ## To be used with a proxy extraContainer port
    targetPort: 9090

    ## List of IP addresses at which the Prometheus server service is available
    externalIPs: []

    ## Port to expose on each node
    nodePort: 30090

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    ## Service type
    type: ClusterIP

    sessionAffinity: ""

  ## Configuration for creating a separate Service for each statefulset Prometheus replica
  servicePerReplica:
    enabled: false
    annotations: {}

    ## Port for Prometheus Service per replica to listen on
    port: 9090

    ## To be used with a proxy extraContainer port
    targetPort: 9090

    ## Port to expose on each node
    ## Only used if servicePerReplica.type is 'NodePort'
    nodePort: 30091

    ## Loadbalancer source IP ranges
    ## Only used if servicePerReplica.type is "loadbalancer"
    loadBalancerSourceRanges: []
    ## Service type
    type: ClusterIP

  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  ingress:
    enabled: false
    annotations: {}
    labels: {}

    hosts: []

    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
    paths: []

    tls: []

  ingressPerReplica:
    enabled: false
    annotations: {}
    labels: {}

    hostPrefix: ""
    ## Domain that will be used for the per replica ingress
    hostDomain: ""

    ## Paths to use for ingress rules
    paths: []

    ## Secret name containing the TLS certificate for Prometheus per replica ingress
    ## Secret must be manually created in the namespace
    tlsSecretName: ""

  ## Configure additional options for default pod security policy for Prometheus
  podSecurityPolicy:
    allowedCapabilities: []

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    selfMonitor: true

    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""

    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    tlsConfig: {}

    bearerTokenFile:

    ## 	metric relabel configs to apply to samples before ingestion.
    metricRelabelings: []

    relabelings: []

  prometheusSpec:
    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
    disableCompaction: false
    ## APIServerConfig
    apiserverConfig: {}

    ## Interval between consecutive scrapes.
    scrapeInterval: ""

    ## Interval between consecutive evaluations.
    evaluationInterval: ""

    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
    listenLocal: false

    enableAdminAPI: false

    ## Image of Prometheus.
    image:
      repository: quay.io/prometheus/prometheus
      tag: v2.15.2

    ## Tolerations for use with node taints
    tolerations: []

    alertingEndpoints: []

    externalLabels: {}

    ## Name of the external label used to denote replica name
    replicaExternalLabelName: ""

    ## If true, the Operator won't add the external label used to denote replica name
    replicaExternalLabelNameClear: false

    ## Name of the external label used to denote Prometheus instance name
    prometheusExternalLabelName: ""

    ## If true, the Operator won't add the external label used to denote Prometheus instance name
    prometheusExternalLabelNameClear: false

    ## External URL at which Prometheus will be reachable.
    externalUrl: ""

    nodeSelector: {}

    secrets: []

    configMaps: []

    ## QuerySpec defines the query command line flags when starting Prometheus.
    query: {}

    ruleNamespaceSelector: {}

    ruleSelectorNilUsesHelmValues: true

    ruleSelector: {}

    serviceMonitorSelectorNilUsesHelmValues: true

    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}

    podMonitorSelectorNilUsesHelmValues: true

    ## PodMonitors to be selected for target discovery.
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}

    ## How long to retain metrics
    retention: 10d

    ## Maximum size of metrics
    retentionSize: ""

    ## Enable compression of the write-ahead log using Snappy.
    ##
    walCompression: false

    ## If true, the Operator won't process any Prometheus configuration changes
    paused: false

    ## Number of Prometheus replicas desired
    replicas: 1

    ## Log level for Prometheus be configured in
    logLevel: info

    ## Log format for Prometheus be configured in
    logFormat: logfmt

    ## Prefix used to register routes, overriding externalUrl route.
    ## Useful for proxies that rewrite URLs.
    routePrefix: /

    podMetadata: {}

    podAntiAffinity: ""

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Assign custom affinity rules to the prometheus instance
    affinity: {}
    remoteRead: []
    remoteWrite: []
    remoteWriteDashboards: false

    ## Resource limits & requests
    resources: {}
    storageSpec: {}
    additionalScrapeConfigs: []
    additionalPrometheusSecretsAnnotations: {}
    additionalAlertManagerConfigs: []
    additionalAlertRelabelConfigs: []

    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000

    priorityClassName: ""

    thanos: {}

    containers: []

    additionalScrapeConfigsExternal: false

    ## PortName to use for Prometheus.
    portName: "web"

  additionalServiceMonitors: []
  additionalPodMonitors: []